# Efficient FPGA-based Accelerator for Post-Processing in Object Detection
The fast object detection algorithms such as YOLO, etc., have two main components: convolutional neural network (CNN) and post-processing. 
In this repository, we upload an efficient FPGA-based accelerator for the post-processing of YOLO object detection. We schedule a pipelined data path for post-processing, which fuses data scanning/caching, decoding, class identification, and non-maximum suppression (NMS) operations. This data path effectively hides the processing time of operations without data dependencies, providing up to 43$\times$ speedup over primitive serial processes. Moreover, We propose a parallel hardware architecture for NMS, allowing for parallel intersection over union (IoU) computation and threshold comparison. This architecture significantly reduces the latency of NMS, providing up to 811$\times$ speedup over primitive serial processes.
Our accelerator implemented on a Xilinx Virtex-7 690t FPGA runs at 150MHz. Impressively, our accelerator achieves minimal latency of only 0.19$\mu$s and a processing time of merely 4.46$\mu$s for the post-processing of the YOLOv3Tiny algorithm. This represents an 378$\times$ speedup compared to the Intel i7-8700 CPU with 3.2GHz. Additionally, our NMS unit exhibits a latency of 0.07$\mu$s, which is nearly 2$\times$ faster than the state-of-the-art FPGA acceleration method. 